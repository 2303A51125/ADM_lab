{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVWjdm9e5yMboYh3Msxc6O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51125/ADM_lab/blob/main/projectcode01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "INPATH = \"/content/academic Stress level - maintainance 1 (1).csv\"\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "def load_and_clean(path):\n",
        "    df = pd.read_csv(path)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "def basic_preprocess(df):\n",
        "    df = df.dropna(axis=1, how=\"all\")\n",
        "    target_col = None\n",
        "    for candidate in ['stress','Stress','stress_level','StressLevel','Stress_Level','target','label']:\n",
        "        if candidate in df.columns:\n",
        "            target_col = candidate\n",
        "            break\n",
        "    if target_col is None:\n",
        "        for col in df.columns:\n",
        "            if df[col].nunique() <= 5:\n",
        "                target_col = col\n",
        "                break\n",
        "    if target_col is None:\n",
        "        raise ValueError(\"Unable to identify target column automatically.\")\n",
        "    X = df.drop(columns=[target_col]).copy()\n",
        "    y_raw = df[target_col].copy()\n",
        "    for c in X.select_dtypes(include=['object']).columns:\n",
        "        X[c] = X[c].astype(str).str.strip().replace({'': np.nan, 'nan': np.nan})\n",
        "    if y_raw.dtype == 'O' or not np.issubdtype(y_raw.dtype, np.number):\n",
        "        le = LabelEncoder()\n",
        "        y = le.fit_transform(y_raw.astype(str))\n",
        "    else:\n",
        "        if y_raw.nunique() > 6:\n",
        "            y = pd.qcut(y_raw, q=3, labels=False, duplicates='drop')\n",
        "            y = y.astype(int).values\n",
        "        else:\n",
        "            y = y_raw.astype(int).values\n",
        "    return X, y, target_col\n",
        "\n",
        "def feature_engineer(X):\n",
        "    X2 = X.copy()\n",
        "    for c in X2.columns:\n",
        "        if X2[c].dtype == 'O':\n",
        "            cleaned = X2[c].astype(str).str.replace(r'[^0-9.\\-]', '', regex=True)\n",
        "            nonempty = cleaned.replace('', np.nan).dropna()\n",
        "            try:\n",
        "                conv = pd.to_numeric(nonempty, errors='coerce')\n",
        "                pct_numeric = conv.notna().sum() / max(1, len(nonempty))\n",
        "            except Exception:\n",
        "                pct_numeric = 0\n",
        "            if pct_numeric > 0.6:\n",
        "                X2[c] = pd.to_numeric(cleaned, errors='coerce')\n",
        "    if 'study_hours' in X2.columns and 'sleep_hours' in X2.columns:\n",
        "        X2['study_sleep_ratio'] = X2['study_hours'] / (X2['sleep_hours'] + 1e-6)\n",
        "    if 'assignments' in X2.columns and 'study_hours' in X2.columns:\n",
        "        X2['assign_per_study'] = X2['assignments'] / (X2['study_hours'] + 1e-6)\n",
        "    return X2\n",
        "\n",
        "def prepare_pipeline_and_split(X, y, test_size=0.2):\n",
        "    X_proc = X.copy()\n",
        "    low_card_cols = [c for c in X_proc.select_dtypes(include=['object','category']).columns if X_proc[c].nunique() <= 20]\n",
        "    if low_card_cols:\n",
        "        X_proc = pd.get_dummies(X_proc, columns=low_card_cols, dummy_na=True)\n",
        "    high_card_cols = [c for c in X_proc.select_dtypes(include=['object','category']).columns if c not in low_card_cols]\n",
        "    X_proc = X_proc.drop(columns=high_card_cols)\n",
        "    for c in X_proc.columns:\n",
        "        if X_proc[c].dtype.kind in 'biufc':\n",
        "            X_proc[c] = X_proc[c].fillna(X_proc[c].median())\n",
        "        else:\n",
        "            X_proc[c] = X_proc[c].fillna(0)\n",
        "    X_matrix = X_proc.values.astype(float)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_matrix, y, test_size=test_size, stratify=y, random_state=RANDOM_STATE)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler, X_proc.columns.tolist()\n",
        "\n",
        "def run_models(X_train, X_test, y_train, y_test):\n",
        "    models = {\n",
        "        \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
        "        \"DecisionTree\": DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
        "        \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE),\n",
        "        \"ExtraTrees\": ExtraTreesClassifier(n_estimators=200, random_state=RANDOM_STATE),\n",
        "        \"GradientBoosting\": GradientBoostingClassifier(n_estimators=300, random_state=RANDOM_STATE),\n",
        "        \"AdaBoost\": AdaBoostClassifier(n_estimators=200, random_state=RANDOM_STATE),\n",
        "        \"KNN\": KNeighborsClassifier(n_neighbors=7),\n",
        "        \"SVM\": SVC(kernel='rbf', probability=False, random_state=RANDOM_STATE),\n",
        "        \"NaiveBayes\": GaussianNB()\n",
        "    }\n",
        "    results = {}\n",
        "    reports = {}\n",
        "    cms = {}\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, preds)\n",
        "        results[name] = acc\n",
        "        reports[name] = classification_report(y_test, preds, output_dict=True)\n",
        "        cms[name] = confusion_matrix(y_test, preds)\n",
        "        print(\"\\n=== Model:\", name, \"===\")\n",
        "        print(\"Accuracy:\", acc)\n",
        "        print(classification_report(y_test, preds, digits=4))\n",
        "    try:\n",
        "        from xgboost import XGBClassifier\n",
        "        xgb = XGBClassifier(n_estimators=300, use_label_encoder=False, eval_metric='mlogloss', random_state=RANDOM_STATE)\n",
        "        xgb.fit(X_train, y_train)\n",
        "        preds = xgb.predict(X_test)\n",
        "        acc = accuracy_score(y_test, preds)\n",
        "        results['XGBoost'] = acc\n",
        "        reports['XGBoost'] = classification_report(y_test, preds, output_dict=True)\n",
        "        cms['XGBoost'] = confusion_matrix(y_test, preds)\n",
        "        print(\"\\n=== Model: XGBoost ===\")\n",
        "        print(\"Accuracy:\", acc)\n",
        "        print(classification_report(y_test, preds, digits=4))\n",
        "    except Exception:\n",
        "        pass\n",
        "    return results, reports, cms\n",
        "\n",
        "def main():\n",
        "    df = load_and_clean(INPATH)\n",
        "    X, y, target_col = basic_preprocess(df)\n",
        "    X_fe = feature_engineer(X)\n",
        "    X_train, X_test, y_train, y_test, scaler, feature_names = prepare_pipeline_and_split(X_fe, y)\n",
        "    results, reports, cms = run_models(X_train, X_test, y_train, y_test)\n",
        "    print(\"\\n=== Summary Accuracies ===\")\n",
        "    for k, v in sorted(results.items(), key=lambda kv: kv[1], reverse=True):\n",
        "        print(f\"{k:15s} : {v:.4f}\")\n",
        "    out_dir = \"supervised_results\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    pd.DataFrame.from_dict(results, orient='index', columns=['accuracy']).to_csv(os.path.join(out_dir, \"accuracy_summary.csv\"))\n",
        "    for model_name, rep in reports.items():\n",
        "        rep_df = pd.DataFrame(rep).transpose()\n",
        "        rep_df.to_csv(os.path.join(out_dir, f\"classification_report_{model_name}.csv\"))\n",
        "    for model_name, cm in cms.items():\n",
        "        cm_df = pd.DataFrame(cm)\n",
        "        cm_df.to_csv(os.path.join(out_dir, f\"confusion_matrix_{model_name}.csv\"))\n",
        "    print(\"\\nSaved results under folder:\", out_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dzTEKIxvUIB",
        "outputId": "a9e35cf0-c4d3-449d-d246-d4f2908b9709"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Model: LogisticRegression ===\n",
            "Accuracy: 0.6428571428571429\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000         6\n",
            "           1     0.0000    0.0000    0.0000         2\n",
            "           2     0.7200    0.9000    0.8000        20\n",
            "\n",
            "    accuracy                         0.6429        28\n",
            "   macro avg     0.2400    0.3000    0.2667        28\n",
            "weighted avg     0.5143    0.6429    0.5714        28\n",
            "\n",
            "\n",
            "=== Model: DecisionTree ===\n",
            "Accuracy: 0.7142857142857143\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6667    0.6667    0.6667         6\n",
            "           1     0.0000    0.0000    0.0000         2\n",
            "           2     0.8000    0.8000    0.8000        20\n",
            "\n",
            "    accuracy                         0.7143        28\n",
            "   macro avg     0.4889    0.4889    0.4889        28\n",
            "weighted avg     0.7143    0.7143    0.7143        28\n",
            "\n",
            "\n",
            "=== Model: RandomForest ===\n",
            "Accuracy: 0.8571428571428571\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.6667    0.8000         6\n",
            "           1     0.0000    0.0000    0.0000         2\n",
            "           2     0.8333    1.0000    0.9091        20\n",
            "\n",
            "    accuracy                         0.8571        28\n",
            "   macro avg     0.6111    0.5556    0.5697        28\n",
            "weighted avg     0.8095    0.8571    0.8208        28\n",
            "\n",
            "\n",
            "=== Model: ExtraTrees ===\n",
            "Accuracy: 0.7142857142857143\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.1667    0.2857         6\n",
            "           1     0.0000    0.0000    0.0000         2\n",
            "           2     0.7917    0.9500    0.8636        20\n",
            "\n",
            "    accuracy                         0.7143        28\n",
            "   macro avg     0.5972    0.3722    0.3831        28\n",
            "weighted avg     0.7798    0.7143    0.6781        28\n",
            "\n",
            "\n",
            "=== Model: GradientBoosting ===\n",
            "Accuracy: 0.8214285714285714\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.8333    0.9091         6\n",
            "           1     0.0000    0.0000    0.0000         2\n",
            "           2     0.9000    0.9000    0.9000        20\n",
            "\n",
            "    accuracy                         0.8214        28\n",
            "   macro avg     0.6333    0.5778    0.6030        28\n",
            "weighted avg     0.8571    0.8214    0.8377        28\n",
            "\n",
            "\n",
            "=== Model: AdaBoost ===\n",
            "Accuracy: 0.6428571428571429\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.3333    0.5000         6\n",
            "           1     0.0000    0.0000    0.0000         2\n",
            "           2     0.7273    0.8000    0.7619        20\n",
            "\n",
            "    accuracy                         0.6429        28\n",
            "   macro avg     0.5758    0.3778    0.4206        28\n",
            "weighted avg     0.7338    0.6429    0.6514        28\n",
            "\n",
            "\n",
            "=== Model: KNN ===\n",
            "Accuracy: 0.7142857142857143\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000         6\n",
            "           1     0.0000    0.0000    0.0000         2\n",
            "           2     0.7143    1.0000    0.8333        20\n",
            "\n",
            "    accuracy                         0.7143        28\n",
            "   macro avg     0.2381    0.3333    0.2778        28\n",
            "weighted avg     0.5102    0.7143    0.5952        28\n",
            "\n",
            "\n",
            "=== Model: SVM ===\n",
            "Accuracy: 0.7142857142857143\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000         6\n",
            "           1     0.0000    0.0000    0.0000         2\n",
            "           2     0.7143    1.0000    0.8333        20\n",
            "\n",
            "    accuracy                         0.7143        28\n",
            "   macro avg     0.2381    0.3333    0.2778        28\n",
            "weighted avg     0.5102    0.7143    0.5952        28\n",
            "\n",
            "\n",
            "=== Model: NaiveBayes ===\n",
            "Accuracy: 0.39285714285714285\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2727    1.0000    0.4286         6\n",
            "           1     0.0000    0.0000    0.0000         2\n",
            "           2     0.8333    0.2500    0.3846        20\n",
            "\n",
            "    accuracy                         0.3929        28\n",
            "   macro avg     0.3687    0.4167    0.2711        28\n",
            "weighted avg     0.6537    0.3929    0.3666        28\n",
            "\n",
            "\n",
            "=== Model: XGBoost ===\n",
            "Accuracy: 0.8571428571428571\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    1.0000    1.0000         6\n",
            "           1     0.0000    0.0000    0.0000         2\n",
            "           2     0.9000    0.9000    0.9000        20\n",
            "\n",
            "    accuracy                         0.8571        28\n",
            "   macro avg     0.6333    0.6333    0.6333        28\n",
            "weighted avg     0.8571    0.8571    0.8571        28\n",
            "\n",
            "\n",
            "=== Summary Accuracies ===\n",
            "RandomForest    : 0.8571\n",
            "XGBoost         : 0.8571\n",
            "GradientBoosting : 0.8214\n",
            "DecisionTree    : 0.7143\n",
            "ExtraTrees      : 0.7143\n",
            "KNN             : 0.7143\n",
            "SVM             : 0.7143\n",
            "LogisticRegression : 0.6429\n",
            "AdaBoost        : 0.6429\n",
            "NaiveBayes      : 0.3929\n",
            "\n",
            "Saved results under folder: supervised_results\n"
          ]
        }
      ]
    }
  ]
}